{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "#Settings.llms=ChatGroq(model=\"llama3-8b-8192\")\n",
    "Settings.llm=Groq(model=\"llama3-70b-8192\")\n",
    "# embedding model\n",
    "Settings.embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "if 'index_store_dev' not in os.listdir():\n",
    "\n",
    "    documents = SimpleDirectoryReader(\"./hack4\", recursive = True).load_data()\n",
    "\n",
    "    # chunk_size of 1024 is a good default value\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    # Create nodes from documents\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    # vector store index\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "    os.makedirs(\"./index_store_dev/vector\", exist_ok=True)\n",
    "\n",
    "\n",
    "    vector_index.storage_context.persist(persist_dir=\"./index_store_dev/vector\")\n",
    "\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    vector_storage_context = StorageContext.from_defaults(persist_dir=\"./index_store_dev/vector\")\n",
    "\n",
    "    # load index\n",
    "    vector_index = load_index_from_storage(vector_storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vector query engine\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        prompt_template = \"\"\"Text: {context}\\n\\n        Question: {question}\\n\\n        Answer the question based on the PDF Document provided. If the text doesn\\'t contain the answer, reply that the answer is not available.\\n        Do Not Hallucinate\"\"\"\\n\\n\\n        PROMPT = PromptTemplate(\\n            template=prompt_template, input_variables=[\"context\", \"query\"]\\n        )'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "        prompt_template = \"\"\"Text: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer the question based on the PDF Document provided. If the text doesn't contain the answer, reply that the answer is not available.\n",
    "        Do Not Hallucinate\"\"\"\n",
    "\n",
    "\n",
    "        PROMPT = PromptTemplate(\n",
    "            template=prompt_template, input_variables=[\"context\", \"query\"]\n",
    "        )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"Tamil Nadu query engine\",\n",
    "            description=(\n",
    "                \"Provides information about Tamil Nadu's services in great detail and mentions all the schemes etc available for the user \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent = ReActAgent.from_tools(\\n    query_engine_tools,\\n    llm=llm,\\n    verbose=True,\\n    \\n)'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "'''agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    \n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "agent = OpenAIAgent.from_tools(query_engine_tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tnsocialwelfare.tn.gov.in/en\n",
      "https://spc.tn.gov.in/policy/state-policy-for-women-2021/\n",
      "https://www.newindianexpress.com/states/tamil-nadu/2024/Feb/22/tn-govts-new-policy-lays-roadmap-for-womens-welfare-for-next-10-years\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "retriever = TavilySearchAPIRetriever(k=3)\n",
    "query = \"women policy \"\n",
    "\n",
    "result=retriever.invoke(f'only from official goverment website of tamil nadu for the query:{query}')\n",
    "source=[document.metadata['source'] for document in result]\n",
    "for s in source:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: All details regarding the query asked in tamil nadu only and answer it in great detail, dont respond anything that is not from the documents. the query is : What are the services provided by Tamil Nadu in the rural areas? List them.\n",
      "=== Calling Function ===\n",
      "Calling function: Tamil Nadu query engine with args: {\"input\":\"What are the services provided by Tamil Nadu in the rural areas? List them.\"}\n",
      "Got output: Based on the provided context, the services provided by Tamil Nadu in the rural areas include:\n",
      "\n",
      "1. Primary Health Services\n",
      "2. Electricity\n",
      "3. Drinking Water\n",
      "4. Toilets (although the availability is quite low)\n",
      "5. Healthcare facilities, including:\n",
      "   a. Community Health Centers (CHCs)\n",
      "   b. Primary Health Centers (PHCs)\n",
      "   c. Sub-Centers (SCs)\n",
      "\n",
      "These services are mentioned in the context as part of the discussion on scaling up primary health services in rural Tamil Nadu.\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=input(\"Enter the query:\")\n",
    "response=agent.chat(f'All details regarding the query asked in tamil nadu only and answer only what is asked no need to requestion ur own self, dont respond anything that is not from the documents. the query is : {query}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the information provided, the services provided by Tamil Nadu in the rural areas include:\n",
      "\n",
      "1. Primary Health Services\n",
      "2. Electricity\n",
      "3. Drinking Water\n",
      "4. Toilets (although the availability is quite low)\n",
      "5. Healthcare facilities, including:\n",
      "   a. Community Health Centers (CHCs)\n",
      "   b. Primary Health Centers (PHCs)\n",
      "   c. Sub-Centers (SCs)\n",
      "\n",
      "These services are mentioned in the context as part of the discussion on scaling up primary health services in rural Tamil Nadu.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
